import os
import math
import argparse
import random
from dataclasses import dataclass
from typing import List, Tuple

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader
from torchvision import datasets, transforms
import matplotlib.pyplot as plt

def set_seed(seed: int = 1337):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    return seed

def ensure_dir(path: str):
    os.makedirs(path, exist_ok=True)

def save_plot(fig, outpath: str, title: str = ""):
    if title:
        fig.suptitle(title)
    fig.tight_layout()
    fig.savefig(outpath, dpi=150)
    plt.close(fig)

class FFNN(nn.Module):
    def __init__(self):
        super().__init__()
        self.net = nn.Sequential(
            nn.Flatten(),
            nn.Linear(28*28, 128),
            nn.ReLU(),
            nn.Linear(128, 10)
        )
    def forward(self, x): return self.net(x)

class SimpleCNN(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(1, 8, kernel_size=5, stride=1, padding=2)
        self.pool  = nn.MaxPool2d(2,2)
        self.conv2 = nn.Conv2d(8, 16, kernel_size=3, stride=1, padding=1)
        self.fc1   = nn.Linear(16*14*14, 64)
        self.fc2   = nn.Linear(64, 10)
    def forward(self, x):
        x = F.relu(self.conv1(x))
        x = self.pool(x)
        x = F.relu(self.conv2(x))
        x = torch.flatten(x, 1)
        x = F.relu(self.fc1(x))
        x = self.fc2(x)
        return x

def get_mnist_loaders(batch_size=64):
    tfm = transforms.Compose([transforms.ToTensor()])
    train_ds = datasets.MNIST(root="./data", train=True, download=True, transform=tfm)
    test_ds  = datasets.MNIST(root="./data", train=False, download=True, transform=tfm)
    return (
        DataLoader(train_ds, batch_size=batch_size, shuffle=True),
        DataLoader(test_ds,  batch_size=batch_size, shuffle=False)
    )

@dataclass
class TrainLog:
    epochs: List[int]
    losses: List[float]
    test_accs: List[float]

def evaluate(model: nn.Module, loader: DataLoader, device: torch.device) -> float:
    model.eval()
    correct=0; total=0
    with torch.no_grad():
        for x,y in loader:
            x,y = x.to(device), y.to(device)
            logits = model(x)
            pred = logits.argmax(1)
            correct += (pred==y).sum().item()
            total += y.numel()
    return correct/total

def train_model(
    model: nn.Module,
    train_loader: DataLoader,
    test_loader: DataLoader,
    device: torch.device,
    epochs: int = 3,
    lr: float = 0.01,
    title: str = "",
    outputs_dir: str = "./outputs",
    save_prefix: str = "ffnn",
) -> TrainLog:
    model.to(device)
    opt = torch.optim.SGD(model.parameters(), lr=lr)
    log = TrainLog(epochs=[], losses=[], test_accs=[])

    for ep in range(1, epochs+1):
        model.train()
        total_loss=0.0; steps=0
        for x,y in train_loader:
            x,y = x.to(device), y.to(device)
            opt.zero_grad()
            logits = model(x)
            loss = F.cross_entropy(logits, y)
            loss.backward()
            opt.step()
            total_loss += loss.item()
            steps += 1
        avg_loss = total_loss/max(1,steps)
        acc = evaluate(model, test_loader, device)
        log.epochs.append(ep); log.losses.append(avg_loss); log.test_accs.append(acc)
        print(f"[{save_prefix}] epoch={ep} loss={avg_loss:.4f} acc={acc:.4f}")

    fig1 = plt.figure()
    plt.plot(log.epochs, log.losses, marker="o")
    plt.xlabel("Epoch"); plt.ylabel("Training Loss"); plt.title(f"{title} — Loss")
    save_plot(fig1, os.path.join(outputs_dir, f"{save_prefix}_loss.png"))

    fig2 = plt.figure()
    plt.plot(log.epochs, log.test_accs, marker="o")
    plt.xlabel("Epoch"); plt.ylabel("Test Accuracy"); plt.title(f"{title} — Test Accuracy")
    save_plot(fig2, os.path.join(outputs_dir, f"{save_prefix}_acc.png"))
    return log

def task_ffnn(outputs_dir: str):
    set_seed()
    train_loader, test_loader = get_mnist_loaders(batch_size=128)
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = FFNN()
    _ = train_model(model, train_loader, test_loader, device,
                    epochs=3, lr=0.05, title="FFNN on MNIST",
                    outputs_dir=outputs_dir, save_prefix="ffnn")

def visualize_first_layer_filters(model: SimpleCNN, outputs_dir: str):
    with torch.no_grad():
        w = model.conv1.weight.cpu()
        fmin = w.min(dim=-1, keepdim=True)[0].min(dim=-2, keepdim=True)[0]
        fmax = w.max(dim=-1, keepdim=True)[0].max(dim=-2, keepdim=True)[0]
        w_norm = (w - fmin) / (fmax - fmin + 1e-8)

        cols = w_norm.shape[0]
        fig, axes = plt.subplots(1, cols, figsize=(2*cols, 2))
        if cols == 1:
            axes = [axes]
        for i in range(cols):
            axes[i].imshow(w_norm[i,0].numpy(), cmap="gray")
            axes[i].set_title(f"F{i}")
            axes[i].axis("off")
        save_plot(fig, os.path.join(outputs_dir, "cnn_filters_conv1.png"), title="First-layer conv filters")

def task_cnn(outputs_dir: str):
    set_seed()
    train_loader, test_loader = get_mnist_loaders(batch_size=128)
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = SimpleCNN()
    _ = train_model(model, train_loader, test_loader, device,
                    epochs=3, lr=0.01, title="CNN on MNIST",
                    outputs_dir=outputs_dir, save_prefix="cnn")
    visualize_first_layer_filters(model, outputs_dir)

def task_sweep(outputs_dir: str):
    set_seed()
    lrs = [0.005, 0.05, 0.5]
    train_loader, test_loader = get_mnist_loaders(batch_size=128)
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    logs = {}
    for lr in lrs:
        model = FFNN()
        logs[lr] = train_model(model, train_loader, test_loader, device,
                               epochs=3, lr=lr, title=f"FFNN (lr={lr})",
                               outputs_dir=outputs_dir, save_prefix=f"sweep_lr_{str(lr).replace('.','_')}")
    fig = plt.figure()
    for lr in lrs:
        plt.plot(logs[lr].epochs, logs[lr].losses, marker="o", label=f"lr={lr}")
    plt.xlabel("Epoch"); plt.ylabel("Training Loss"); plt.title("Learning rate sweep — Training loss")
    plt.legend()
    save_plot(fig, os.path.join(outputs_dir, "sweep_loss_compare.png"))

def conv_out_size(H: int, W: int, k: int, s: int, p: int, d: int=1) -> Tuple[int,int]:
    def one_dim(n):
        return math.floor((n + 2*p - d*(k-1) - 1)/s + 1)
    return one_dim(H), one_dim(W)

def task_convarith(outputs_dir: str):
    set_seed()
    cases = [
        (28,28,5,1,2,1),
        (28,28,3,2,1,1),
        (32,32,3,1,0,1),
        (64,64,7,2,3,1),
        (64,64,3,1,1,2),
    ]
    rows = []
    for (H,W,k,s,p,d) in cases:
        h_out, w_out = conv_out_size(H,W,k,s,p,d)
        x = torch.randn(1,1,H,W)
        conv = nn.Conv2d(1, 1, kernel_size=k, stride=s, padding=p, dilation=d)
        with torch.no_grad():
            y = conv(x)
        rows.append((H,W,k,s,p,d,h_out,w_out, y.shape[-2], y.shape[-1]))
        print(f"[conv] in=({H},{W}) k={k} s={s} p={p} d={d} -> formula=({h_out},{w_out}), torch={tuple(y.shape[-2:])}")

    fig = plt.figure(figsize=(8,4))
    plt.axis("off")
    text = "H  W | k s p d | formula(H,W) | torch(H,W)
" + "-"*48 + "\n"
    for r in rows:
        text += f"{r[0]:>2} {r[1]:>2} | {r[2]} {r[3]} {r[4]} {r[5]} | ({r[6]:>2},{r[7]:>2}) | ({r[8]:>2},{r[9]:>2})\n"
    plt.text(0.02, 0.98, text, va="top", family="monospace")
    save_plot(fig, os.path.join(outputs_dir, "conv_arith_checks.png"), title="Convolution arithmetic checks")

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--tasks", nargs="*", default=["ffnn","cnn","sweep","convarith"])
    parser.add_argument("--outputs", type=str, default="./outputs")
    args = parser.parse_args()

    ensure_dir(args.outputs)
    if "ffnn" in args.tasks: task_ffnn(args.outputs)
    if "cnn" in args.tasks: task_cnn(args.outputs)
    if "sweep" in args.tasks: task_sweep(args.outputs)
    if "convarith" in args.tasks: task_convarith(args.outputs)

if __name__ == "__main__":
    main()
